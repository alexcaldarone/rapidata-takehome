<p align="center">
  <img src="assets/rapidata_logo.png" alt="Description" width="100"/>
</p>

# Rapidata Take-Home Task

### 1. Goals
In this task I conducted data collection, analysis and modelling for two possible use cases related to Rapidata's platform. 

By selecting images generated by AI models and using Rapidata's API to compare them I gather information about user model preference, which is then analyzed. I then focus on two modelling questions which are relevant to Rapidata's platform:
- Can we predict whether a user's vote is correct?
- How do we rank these models?

The first of these questions is of particular interest to Rapidata, as understanding the user base and their voting patterns (i.e. if their votes are correct/useful) is crucial in making sure that we can provide the best possible service to clients who rely on the product to train their models. Practical impacts of these insights could be excluding certain users from the voting process, or weighting their votes differently based on their voting history.

As for the second question, ranking the models is important for Rapidata to understand which models are preferred by users and how they compare to each other (and also for the model ranking on the website).

### 2. Data collection and modelling

First, a subset of images and prompts were selected from [this dataset](https://huggingface.co/datasets/Rapidata/human-style-preferences-images). Images can be found in the `images/` folder and the respective prompts in the `data/prompts.json` file. The models compared are:
- OpenAI 4o
- Halfmoon
- Stable Diffusion
- DALLE 3

Then, image pairs were generated for each prompt in order to compare each model against the others. These pairs were then sent for human evaluation through Rapidata's API. The results are stored in `data/results.json`.

Then in `rapidata_takehome.ipynb` I analyze the data and, in order to answer the first question posed, build a model to predict whether a user's vote is correct. Different models are tried and the perfomance of each of them is evaluated on the test set. For a more detailed discussion of the results see the Jupyter notebook.

As for the Bradley-Terry model, this is implemented from scratch in the `src/bradley_terry.py` file using numpy. The model is estimated on the data collected and provides a ranking that is consistent with what we observe in the dataset.

### 3. Extensions to real-world scenario

The models and data analysis conducted here were done on a simple dataset. Though these address questions that are relevant to Rapidata. Some extensions of these to a more real-world scenario could be:

- Using a much larger dataset consisting of the user votes on different tasks, comparing different models etc...
- Implementing a ML model in production which could predict in real time the quality of a user's vote (where quality of vote is to be defined...). This would allow Rapidata to filter out bad votes and improve the quality of the data collected. Also, this could also help Rapidata improve the score it already assigns to users (assuming this is not done yet...). More generally, this is a first step in what could be a larger ML pipeline to implement quality control of the human annotations collected.

### 4. Conclusion

This task was mainly focused on the data collection and analysis of the data collected. The modelling part was not as extensive as I would have liked, but I think it provides a good starting point and is an example of tasks that are relevant to Rapidata.
Partly due to time constraints, I was not able to perform more extensive analysis, though one idea I had was to implement a quick dashboard that would allow users to upload two images, asking users to assign them a score (for example, likert scale, or asking to assign emotions) with the instructions formulated differently and then display the results. This would have been a nice way to explore in real time how different instructions impact the results and how the users perceive the images.

### 5. Suggestions for Rapidata and future work

As for the usability of the prouct itself, I found the API quite easy to navigate and use. The documentation is relatively clear. The only thing I would suggest (though it could be I missed it) is to mention that for larger workflows (100+) images, approval is needed and you won't be able to get the results immediately. This is not a problem per se, but it would be good to mention it in the documentation (though again, I could have missed it).

Related to the point above (paragraph 3) about quality control, one could also think of implement a user behaviour model. We could use historical data collected on the different annotators to flag potential bad votes, spammers, etc... 

Other suggestions or directions which might be interesting to explore:
- Many tasks and evaluations are redudant or similar. An interesting idea, which would allow for longer labelling times, would be to implement a ML pipeline which would prioritize the labelling of what are the most uncertain/difficult pairs of images etc.. (again, task dependent). Other ``easy" examples could be labelled by another model (e.g. a model trained on the data collected so far). This would allow for a more efficient use of the human annotators and also allow for a better quality of the data collected. (Though this will come with challenges related to sharing data, or implementing a federated learning approach).
- Can we choose what tasks to assign to annotators? In other words, can we build a model that assigns certain tasks to certain annotators who have perfomed quite well (good, consistent resulsts in the past) in order to make sure that we get the best possible results.

Other exciting use cases could come up also based on how Rapidata evolves as a company and a product, the data collected, the type of data that is processed and services offered to client etc...

---
**Alex John Caldarone - 17/04/2025**
